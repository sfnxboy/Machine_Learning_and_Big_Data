{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenize_Data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXJ5U_JjCmnD",
        "outputId": "a7a9f678-dd50-4c67-b8c7-51e22a5a9486"
      },
      "source": [
        "import os\r\n",
        "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\r\n",
        "# For example:\r\n",
        "# spark_version = 'spark-3.0.1'\r\n",
        "spark_version = 'spark-3.0.1'\r\n",
        "os.environ['SPARK_VERSION']=spark_version\r\n",
        "\r\n",
        "# Install Spark and Java\r\n",
        "!apt-get update\r\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\r\n",
        "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\r\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\r\n",
        "!pip install -q findspark\r\n",
        "\r\n",
        "# Set Environment Variables\r\n",
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\r\n",
        "\r\n",
        "# Start a SparkSession\r\n",
        "import findspark\r\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Waiting for heade\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r                                                                               \rHit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Connecting to ppa\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Connecting to cloud.r-proj\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Connecting to cloud.r-proj\r                                                                               \rHit:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Ign:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:8 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja72lbXMC0RL"
      },
      "source": [
        "# Start Spark session\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.appName(\"Tokens\").getOrCreate()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OZGUZqXC2yP"
      },
      "source": [
        "# Import the Tokenizer library\r\n",
        "from pyspark.ml.feature import Tokenizer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SVBiaARC62E",
        "outputId": "efc93535-d6d0-4112-c6a0-1009bed948c4"
      },
      "source": [
        "# Create sample Dataframe\r\n",
        "dataframe = spark.createDataFrame([\r\n",
        "                                   (0, \"My name is Amir ElTabakh\"),\r\n",
        "                                   (1, \"I am a junior in college at this time\"),\r\n",
        "                                   (2, \"I am interning at NASA\")\r\n",
        "], [\"id\", \"sentence\"])\r\n",
        "\r\n",
        "dataframe.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "| id|            sentence|\n",
            "+---+--------------------+\n",
            "|  0|My name is Amir E...|\n",
            "|  1|I am a junior in ...|\n",
            "|  2|I am interning at...|\n",
            "+---+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3-4iPXNDaey"
      },
      "source": [
        "The tokenizer function takes input and output parameters. The input passes the name of the column that we want to have tokenized, and the output takes the name that we want the column called. Type and run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlgpENfTDdh-",
        "outputId": "abb79939-bbe4-46ec-daf9-254d76d1c8a7"
      },
      "source": [
        "# Tokenize sentences\r\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\r\n",
        "tokenizer"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer_3dd4a873d50f"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDVUWjCsDiHl"
      },
      "source": [
        "The tokenizer that we created uses a transform method that takes a DataFrame as input. This is a transformation, so to reveal the results, we'll call show(truncate=False) as our action to display the results without shortening the output, as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7hSCKCXDfVd",
        "outputId": "f5406af6-f006-4c24-8845-83306c287556"
      },
      "source": [
        "# Transform and show DataFrame\r\n",
        "tokenized_df = tokenizer.transform(dataframe)\r\n",
        "tokenized_df.show(truncate=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------------------------------------+-----------------------------------------------+\n",
            "|id |sentence                             |words                                          |\n",
            "+---+-------------------------------------+-----------------------------------------------+\n",
            "|0  |My name is Amir ElTabakh             |[my, name, is, amir, eltabakh]                 |\n",
            "|1  |I am a junior in college at this time|[i, am, a, junior, in, college, at, this, time]|\n",
            "|2  |I am interning at NASA               |[i, am, interning, at, nasa]                   |\n",
            "+---+-------------------------------------+-----------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wX-thWkDx8S"
      },
      "source": [
        "The tokenizer looks similar to the split() method in Python. We can create a function that will enhance our tokenizer by returning a word count for each line. Start by creating a Python function that takes a list of words as its input, then returns the length of that list. We'll also import the `udf` function, the `col` function to select a column to be passed into a function, and the type IntegerType that will be used in our udf to define the data type of the output. We can then redo the tokenizer process. Only this time, after the DataFrame has outputted the tokenized values, we can use our own created function to return the number of tokens created. This will give us another data point to use in the future, if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tDm9kQ-DsQt"
      },
      "source": [
        "# Create a function to return the length of a list\r\n",
        "def word_list_length(word_list):\r\n",
        "    return len(word_list)\r\n",
        "\r\n",
        "from pyspark.sql.functions import col, udf\r\n",
        "from pyspark.sql.types import IntegerType\r\n",
        "\r\n",
        "# Create a user defined function\r\n",
        "count_tokens = udf(word_list_length, IntegerType())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XFcLPS8D7eX",
        "outputId": "2569d4fe-560b-4e53-e2f6-61a2fafe820a"
      },
      "source": [
        "# Create our tokenizer\r\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\r\n",
        "\r\n",
        "# Transform DataFrame\r\n",
        "tokenized_df = tokenizer.transform(dataframe)\r\n",
        "\r\n",
        "# Select the needed columns and don't truncate results\r\n",
        "tokenized_df.withColumn(\"tokens\", count_tokens(col(\"words\"))).show(truncate=False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------------------------------------+-----------------------------------------------+------+\n",
            "|id |sentence                             |words                                          |tokens|\n",
            "+---+-------------------------------------+-----------------------------------------------+------+\n",
            "|0  |My name is Amir ElTabakh             |[my, name, is, amir, eltabakh]                 |5     |\n",
            "|1  |I am a junior in college at this time|[i, am, a, junior, in, college, at, this, time]|9     |\n",
            "|2  |I am interning at NASA               |[i, am, interning, at, nasa]                   |5     |\n",
            "+---+-------------------------------------+-----------------------------------------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}