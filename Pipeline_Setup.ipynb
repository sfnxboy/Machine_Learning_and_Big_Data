{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pipeline_Setup.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fi0MM5-H76h"
      },
      "source": [
        "A pipeline enables us to store all of the functions we have created in different stages and run only once. Each stage that is passed in won't run until the previous stage has been completed, which means the output of one stage is passed on to the next one.\r\n",
        "\r\n",
        "We'll put the pipeline to use by employing a sample Yelp dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk74bL8MHuIT",
        "outputId": "f7c6797c-9c94-46ba-a93f-cba143418005"
      },
      "source": [
        "import os\r\n",
        "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\r\n",
        "# For example:\r\n",
        "# spark_version = 'spark-3.0.1'\r\n",
        "spark_version = 'spark-3.0.1'\r\n",
        "os.environ['SPARK_VERSION']=spark_version\r\n",
        "\r\n",
        "# Install Spark and Java\r\n",
        "!apt-get update\r\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\r\n",
        "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\r\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\r\n",
        "!pip install -q findspark\r\n",
        "\r\n",
        "# Set Environment Variables\r\n",
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\r\n",
        "\r\n",
        "# Start a SparkSession\r\n",
        "import findspark\r\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [41.5 kB]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,703 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [872 kB]\n",
            "Fetched 2,889 kB in 3s (892 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeH-_wm4Hyng"
      },
      "source": [
        "# Start Spark session\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.appName(\"Yelp_NLP\").getOrCreate()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdi1mjGVHz1t",
        "outputId": "7008c296-8fe6-4c42-df8f-c976f820b666"
      },
      "source": [
        "# Read in data from S3 Buckets\r\n",
        "from pyspark import SparkFiles\r\n",
        "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_2/yelp_reviews.csv\"\r\n",
        "spark.sparkContext.addFile(url)\r\n",
        "df = spark.read.csv(SparkFiles.get(\"yelp_reviews.csv\"), sep=\",\", header=True)\r\n",
        "\r\n",
        "# Show DataFrame\r\n",
        "df.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+\n",
            "|   class|                text|\n",
            "+--------+--------------------+\n",
            "|positive|Wow... Loved this...|\n",
            "|negative|  Crust is not good.|\n",
            "|negative|Not tasty and the...|\n",
            "|positive|Stopped by during...|\n",
            "|positive|The selection on ...|\n",
            "|negative|Now I am getting ...|\n",
            "|negative|Honeslty it didn'...|\n",
            "|negative|The potatoes were...|\n",
            "|positive|The fries were gr...|\n",
            "|positive|      A great touch.|\n",
            "|positive|Service was very ...|\n",
            "|negative|  Would not go back.|\n",
            "|negative|The cashier had n...|\n",
            "|positive|I tried the Cape ...|\n",
            "|negative|I was disgusted b...|\n",
            "|negative|I was shocked bec...|\n",
            "|positive| Highly recommended.|\n",
            "|negative|Waitress was a li...|\n",
            "|negative|This place is not...|\n",
            "|negative|did not like at all.|\n",
            "+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMFDyEeKH1QT"
      },
      "source": [
        "# Import functions\r\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZKl2tKkINpa"
      },
      "source": [
        "create a new column that uses the lengthfunction to create a future feature with the length of each row. This is similar to the tokenizerphase when we created our own udfto do the same thing. A udfcould still be used here, but PySpark makes it easier by supplying a ready-to-use function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq73T-CgICIx",
        "outputId": "bb6c8723-a005-43eb-ad7c-299c40712cf9"
      },
      "source": [
        "from pyspark.sql.functions import length\r\n",
        "# Create a length column to be used as a future feature\r\n",
        "data_df = df.withColumn('length', length(df['text']))\r\n",
        "data_df.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+------+\n",
            "|   class|                text|length|\n",
            "+--------+--------------------+------+\n",
            "|positive|Wow... Loved this...|    24|\n",
            "|negative|  Crust is not good.|    18|\n",
            "|negative|Not tasty and the...|    41|\n",
            "|positive|Stopped by during...|    87|\n",
            "|positive|The selection on ...|    59|\n",
            "|negative|Now I am getting ...|    46|\n",
            "|negative|Honeslty it didn'...|    37|\n",
            "|negative|The potatoes were...|   111|\n",
            "|positive|The fries were gr...|    25|\n",
            "|positive|      A great touch.|    14|\n",
            "|positive|Service was very ...|    24|\n",
            "|negative|  Would not go back.|    18|\n",
            "|negative|The cashier had n...|    99|\n",
            "|positive|I tried the Cape ...|    59|\n",
            "|negative|I was disgusted b...|    62|\n",
            "|negative|I was shocked bec...|    50|\n",
            "|positive| Highly recommended.|    19|\n",
            "|negative|Waitress was a li...|    38|\n",
            "|negative|This place is not...|    51|\n",
            "|negative|did not like at all.|    20|\n",
            "+--------+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW2n7OAsISbV"
      },
      "source": [
        "Now we'll create all the transformations to be applied in our pipeline. Note that the StringIndexer encodes a string column to a column of table indexes. Here we are working with positive and negative game reviews, which will be converted to 0 and 1. This will form our labels, which we'll delve into in the ML unit. The label is what we're trying to predict: will the review's given text let us know if it was positive or negative?\r\n",
        "\r\n",
        "Also note that we don't need to run all of these completely as we did before. By creating all the functions now, we can then use them all in the pipeline later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DohC734EID-b"
      },
      "source": [
        "# Create all the features to the data set\r\n",
        "pos_neg_to_num = StringIndexer(inputCol='class',outputCol='label')\r\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\r\n",
        "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\r\n",
        "hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\r\n",
        "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO7hWRL4IWXe"
      },
      "source": [
        "We'll create a feature vector containing the output from the IDFModel (the last stage in the pipeline) and the length. This will combine all the raw features to train the ML model that we'll be using. Enter the code for this as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkDZXF_dIURU"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\r\n",
        "from pyspark.ml.linalg import Vector\r\n",
        "# Create feature vectors\r\n",
        "clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFMU9LK4IbIM"
      },
      "source": [
        "Now it's time to create our pipeline, the easiest step. We'll import the pipeline from pyspark.ml, and then store a list of the stages created earlier. It's important to list the stages in the order they need to be executed. As we mentioned before, the output from one stage will then be passed off to another stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWAfFkM6Id2x"
      },
      "source": [
        "# Create and run a data processing Pipeline\r\n",
        "from pyspark.ml import Pipeline\r\n",
        "data_prep_pipeline = Pipeline(stages=[pos_neg_to_num, tokenizer, stopremove, hashingTF, idf, clean_up])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxTOWJC1IlOS"
      },
      "source": [
        "All the stages of the pipeline have been set up. You may have noticed that each individual step didn't need to be set up, which is the perk of setting up the pipeline! Now your data is ready to be run through the pipeline. Then we can run it through a machine learning model.\r\n",
        "\r\n",
        "After our pipeline has been set up, we'll fit the outcome with our original DataFrame and transform it.\r\n",
        "\r\n",
        "As you can see in the following image, our labels and features that we created early on in the process are numerical representations of positive and negative reviews. The features will be used in our model and predict whether a given review will be positive or negative. These features are the result of all the work we have been doing with the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWXYnt3iIwZu",
        "outputId": "f8425fab-5c73-4d9b-9d1c-d0fa97d7b725"
      },
      "source": [
        "# Fit and transform the pipeline\r\n",
        "cleaner = data_prep_pipeline.fit(data_df)\r\n",
        "cleaned = cleaner.transform(data_df)\r\n",
        "\r\n",
        "# Show label and resulting features\r\n",
        "cleaned.select(['label', 'features']).show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  1.0|(262145,[177414,2...|\n",
            "|  0.0|(262145,[49815,23...|\n",
            "|  0.0|(262145,[109649,1...|\n",
            "|  1.0|(262145,[53101,68...|\n",
            "|  1.0|(262145,[15370,77...|\n",
            "|  0.0|(262145,[98142,13...|\n",
            "|  0.0|(262145,[59172,22...|\n",
            "|  0.0|(262145,[63420,85...|\n",
            "|  1.0|(262145,[53777,17...|\n",
            "|  1.0|(262145,[221827,2...|\n",
            "|  1.0|(262145,[43756,22...|\n",
            "|  0.0|(262145,[127310,1...|\n",
            "|  0.0|(262145,[407,3153...|\n",
            "|  1.0|(262145,[18098,93...|\n",
            "|  0.0|(262145,[23071,12...|\n",
            "|  0.0|(262145,[129941,1...|\n",
            "|  1.0|(262145,[19633,21...|\n",
            "|  0.0|(262145,[27707,65...|\n",
            "|  0.0|(262145,[20891,27...|\n",
            "|  0.0|(262145,[8287,208...|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLwe57BJJJba"
      },
      "source": [
        "Now let's run our ML model on the data. One of the basics of ML is that data gets broken into training data and testing data. Training data is the data that will be passed to our NLP model that will train our model to predict results. The testing data is used to test our predictions. We can do this with the randomSplit method, which takes in a list of the percent of data we want split into each group. Standard conventions use 70% with training and 30% with testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueN_UQptIz2v"
      },
      "source": [
        "# Break data down into a training set and a testing set\r\n",
        "training, testing = cleaned.randomSplit([0.7, 0.3], 21)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfXifbYvJRn7"
      },
      "source": [
        "The array supplied to randomSplit is the percentage of the data that will be broken into training and testing respectively. So 70% to training and 30% to testing. The second number supplied is called a seed. The seed number here, 21, is arbitrary. But as long as the same seed is used, the result will be the same each time. Using a seed number ensures reproducible results.\r\n",
        "\r\n",
        "The ML model we'll use is Naive Bayes, which we'll import and then fit the model using the training dataset. Naive Bayes is a group of classifier algorithms based on Bayes' theorem. Bayes theorem provides a way to determine the probability of an event based on new conditions or information that might be related to the event."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvhsL--AJLhm",
        "outputId": "414d2617-3bc0-4b5d-e96e-62211811d213"
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\r\n",
        "# Create a Naive Bayes model and fit training data\r\n",
        "nb = NaiveBayes()\r\n",
        "predictor = nb.fit(training)\r\n",
        "\r\n",
        "# Transform the model with the testing data\r\n",
        "test_results = predictor.transform(testing)\r\n",
        "test_results.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|   class|                text|length|label|          token_text|         stop_tokens|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|\n",
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|negative|\"The burger... I ...|    86|  0.0|[\"the, burger...,...|[\"the, burger...,...|(262144,[20298,21...|(262144,[20298,21...|(262145,[20298,21...|[-820.60780566975...|[0.99999999999995...|       0.0|\n",
            "|negative|              #NAME?|     6|  0.0|            [#name?]|            [#name?]|(262144,[197050],...|(262144,[197050],...|(262145,[197050,2...|[-73.489435340867...|[0.07515735596910...|       1.0|\n",
            "|negative|After I pulled up...|    83|  0.0|[after, i, pulled...|[pulled, car, wai...|(262144,[65645,71...|(262144,[65645,71...|(262145,[65645,71...|[-620.40646705112...|[1.0,1.9205984091...|       0.0|\n",
            "|negative|Also, I feel like...|    58|  0.0|[also,, i, feel, ...|[also,, feel, lik...|(262144,[61899,66...|(262144,[61899,66...|(262145,[61899,66...|[-528.59562125515...|[0.99999999994873...|       0.0|\n",
            "|negative|Anyway, I do not ...|    44|  0.0|[anyway,, i, do, ...|[anyway,, think, ...|(262144,[132270,1...|(262144,[132270,1...|(262145,[132270,1...|[-334.09599709326...|[0.99999999994185...|       0.0|\n",
            "|negative|Anyways, The food...|   102|  0.0|[anyways,, the, f...|[anyways,, food, ...|(262144,[6346,685...|(262144,[6346,685...|(262145,[6346,685...|[-779.27125246078...|[0.99905255158977...|       0.0|\n",
            "|negative|As a sushi lover ...|    47|  0.0|[as, a, sushi, lo...|[sushi, lover, av...|(262144,[6261,709...|(262144,[6261,709...|(262145,[6261,709...|[-400.43450785699...|[0.99999998920319...|       0.0|\n",
            "|negative|As much as I'd li...|    95|  0.0|[as, much, as, i'...|[much, like, go, ...|(262144,[37908,43...|(262144,[37908,43...|(262145,[37908,43...|[-694.99839226333...|[1.0,1.5392272978...|       0.0|\n",
            "|negative|Before I go in to...|   132|  0.0|[before, i, go, i...|[go, gave, 1, sta...|(262144,[9781,926...|(262144,[9781,926...|(262145,[9781,926...|[-1163.1578812603...|[1.0,2.0965497135...|       0.0|\n",
            "|negative|But I definitely ...|    42|  0.0|[but, i, definite...|[definitely, eat,...|(262144,[6346,112...|(262144,[6346,112...|(262145,[6346,112...|[-225.43919775119...|[0.99979792050352...|       0.0|\n",
            "|negative|But now I was com...|    37|  0.0|[but, now, i, was...|[completely, gros...|(262144,[123874,1...|(262144,[123874,1...|(262145,[123874,1...|[-300.28103502273...|[0.93477724115336...|       0.0|\n",
            "|negative|But then they cam...|    29|  0.0|[but, then, they,...| [came, back, cold.]|(262144,[65844,13...|(262144,[65844,13...|(262145,[65844,13...|[-195.30008379866...|[0.99999998229401...|       0.0|\n",
            "|negative|  Crust is not good.|    18|  0.0|[crust, is, not, ...|      [crust, good.]|(262144,[49815,23...|(262144,[49815,23...|(262145,[49815,23...|[-158.39316295644...|[0.24542103246923...|       1.0|\n",
            "|negative|Del Taco is prett...|    59|  0.0|[del, taco, is, p...|[del, taco, prett...|(262144,[16793,21...|(262144,[16793,21...|(262145,[16793,21...|[-570.55820515039...|[3.21792658799547...|       1.0|\n",
            "|negative|Do yourself a fav...|    49|  0.0|[do, yourself, a,...|[favor, stay, awa...|(262144,[1004,912...|(262144,[1004,912...|(262145,[1004,912...|[-401.50632368758...|[2.80032615028215...|       1.0|\n",
            "|negative|Don't bother comi...|    25|  0.0|[don't, bother, c...|[bother, coming, ...|(262144,[12409,31...|(262144,[12409,31...|(262145,[12409,31...|[-225.61607913435...|[0.99953245668819...|       0.0|\n",
            "|negative|Everything was gr...|    21|  0.0|[everything, was,...|[everything, gross.]|(262144,[76187,12...|(262144,[76187,12...|(262145,[76187,12...|[-184.45183122800...|[0.04035071997062...|       1.0|\n",
            "|negative|First - the bathr...|   105|  0.0|[first, -, the, b...|[first, -, bathro...|(262144,[25615,38...|(262144,[25615,38...|(262145,[25615,38...|[-1003.3633555890...|[0.96780198039995...|       0.0|\n",
            "|negative|Food was average ...|    25|  0.0|[food, was, avera...|[food, average, b...|(262144,[38795,98...|(262144,[38795,98...|(262145,[38795,98...|[-191.08411756853...|[0.99999999999994...|       0.0|\n",
            "|negative|Food was really b...|    23|  0.0|[food, was, reall...|[food, really, bo...|(262144,[121133,1...|(262144,[121133,1...|(262145,[121133,1...|[-190.57878406148...|[0.85028925285655...|       0.0|\n",
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGxr0VtxJhcf"
      },
      "source": [
        "This prediction column will indicate with a 1.0 if the model thinks this review is negative and 0.0 if it thinks it's positive. Future data sets can now be run with this model and determine whether a review was positive or negative without having already supplied in the data.\r\n",
        "\r\n",
        "How useful is this model? Should we just blindly trust that it will be right every time? There is one last step in the process to answer these questions.\r\n",
        "\r\n",
        "It's often not enough to simply train and use a machine learning model for predictions without knowing how well the model performs at its prediction task. The last step is to import the BinaryClassificationEvaluator, which will display how accurate our model is in determining if a review with be positive or negative based solely on the text within a review.\r\n",
        "\r\n",
        "The BinaryClassificationEvaluator uses two arguments, labelCol and rawPredictionCol. The labelCol takes the labels which were the result of using StringIndexer to convert our positive and negative strings to integers. The rawPredictionCol takes in numerical predictions from the output of running the Naive Bayes model.\r\n",
        "\r\n",
        "The performance of a model can be measured based on the difference between its predicted values and actual values. This is what the BinaryClassificationEvaluator does. We will dive more into accuracy, precision, and sensitivity when we get to Machine Learning. Run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0UKKUGiJk7T",
        "outputId": "3b5030ad-48dc-4676-c2bc-12c13fef7155"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\r\n",
        "\r\n",
        "acc_eval = BinaryClassificationEvaluator(labelCol = 'label', rawPredictionCol = 'prediction')\r\n",
        "acc = acc_eval.evaluate(test_results)\r\n",
        "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of model at predicting reviews was: 0.700298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frPUVifpKMGd"
      },
      "source": [
        "The accuracy of the model isn't perfect, but it's not too low either: 0.700298. Machine learning isn't a guarantee, and tweaking our models as well as the data used is part of the process. One of the ways to do this is to add more data; when you keep adding data, eventually you grow from your local storage to something much largerâ€”thus leading to big data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHRXmq_XKNKS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}