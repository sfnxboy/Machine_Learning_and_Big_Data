{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stop_Words.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZW2l8aGFV04",
        "outputId": "6da1c1d6-20b3-4051-9e87-2815f9f55062"
      },
      "source": [
        "import os\r\n",
        "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\r\n",
        "# For example:\r\n",
        "# spark_version = 'spark-3.0.1'\r\n",
        "spark_version = 'spark-3.0.1'\r\n",
        "os.environ['SPARK_VERSION']=spark_version\r\n",
        "\r\n",
        "# Install Spark and Java\r\n",
        "!apt-get update\r\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\r\n",
        "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\r\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\r\n",
        "!pip install -q findspark\r\n",
        "\r\n",
        "# Set Environment Variables\r\n",
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\r\n",
        "\r\n",
        "# Start a SparkSession\r\n",
        "import findspark\r\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Waiting for h\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbdvf_pwFatV"
      },
      "source": [
        " # Start Spark session\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.appName(\"StopWords\").getOrCreate()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njxjP8tBFc6u",
        "outputId": "6480f334-fcf4-4932-adce-e4284505a00e"
      },
      "source": [
        "# Create sample Dataframe\r\n",
        "sentenceData = spark.createDataFrame([\r\n",
        "                                   (0, [\"My\", \"name\", \"is\", \"Amir\", \"ElTabakh\"]),\r\n",
        "                                   (1, [\"I\", \"am\", \"a\", \"junior\", \"in\", \"college\", \"at\", \"this\", \"time\"]),\r\n",
        "                                   (2, [\"I\", \"am\", \"interning\", \"at\", \"NASA\"])\r\n",
        "], [\"id\", \"raw\"])\r\n",
        "\r\n",
        "sentenceData.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "| id|                 raw|\n",
            "+---+--------------------+\n",
            "|  0|[My, name, is, Am...|\n",
            "|  1|[I, am, a, junior...|\n",
            "|  2|[I, am, interning...|\n",
            "+---+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJlaA83lFn8J"
      },
      "source": [
        "# Import the StopWordsRemove library\r\n",
        "from pyspark.ml.feature import StopWordsRemover"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yQ1mHT1FtkP"
      },
      "source": [
        "# Run the Remover\r\n",
        "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_cBP1s-GcoV"
      },
      "source": [
        "The StopWordsRemover() function, which takes an input column that will be passed into the function, and an output column to add the results. This is stored in a variable for ease of use. The outcome shows the raw data, or input, and the result of running the StopWordsRemover() function. In the filtered column, all the stop words are removed and the result is displayed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG0lewuKFwvJ",
        "outputId": "129f7e36-32a6-43f4-973a-97d5013b9ce4"
      },
      "source": [
        "# Transform and show data\r\n",
        "remover.transform(sentenceData).show(truncate=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-----------------------------------------------+-----------------------+\n",
            "|id |raw                                            |filtered               |\n",
            "+---+-----------------------------------------------+-----------------------+\n",
            "|0  |[My, name, is, Amir, ElTabakh]                 |[name, Amir, ElTabakh] |\n",
            "|1  |[I, am, a, junior, in, college, at, this, time]|[junior, college, time]|\n",
            "|2  |[I, am, interning, at, NASA]                   |[interning, NASA]      |\n",
            "+---+-----------------------------------------------+-----------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}